<!DOCTYPE html>
<html class="no-js" lang="en-US" prefix="og: http://ogp.me/ns# fb: http://ogp.me/ns/fb#">
<head>
    
<script type="application/javascript">
var doNotTrack = false;
if (!doNotTrack) {
	window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;
	ga('create', 'UA-161158389-1', 'auto');
	
	ga('send', 'pageview');
}
</script>
<script async src='https://www.google-analytics.com/analytics.js'></script>

    <meta charset="utf-8">

    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
<meta name="description" content="">
<meta name="HandheldFriendly" content="True">
<meta name="MobileOptimized" content="320">
<meta name="viewport" content="width=device-width, initial-scale=1">


<meta name="keywords" content="">

 
<meta property="og:type" content="article"/>
<meta property="og:description" content=""/>
<meta property="og:title" content="转发 NLP分词算法分析 : www.jlab.tech"/>
<meta property="og:site_name" content="jLab is Jason Jiao's Lab"/>
<meta property="og:image" content="" />
<meta property="og:image:type" content="image/jpeg" />
<meta property="og:image:width" content="" />
<meta property="og:image:height" content="" />
<meta property="og:url" content="https://www.jlab.tech/post/2016-10-09-jhfnetboy-blog1476008774/">
<meta property="og:locale" content="en_US">
<meta property="article:published_time" content="2016-10-09"/>
<meta property="article:modified_time" content="2016-10-09"/>





<meta name="twitter:card" content="summary">

<meta name="twitter:site" content="@jhfnetboy">
<meta name="twitter:title" content="转发 NLP分词算法分析 : www.jlab.tech">
<meta name="twitter:creator" content="@spf13">
<meta name="twitter:description" content="">
<meta name="twitter:image:src" content="">
<meta name="twitter:domain" content="www.jlab.tech">



    <base href="https://www.jlab.tech/">
    <title> 转发 NLP分词算法分析 - www.jlab.tech </title>
    <link rel="canonical" href="https://www.jlab.tech/post/2016-10-09-jhfnetboy-blog1476008774/">

    <link href='https://fonts.googleapis.com/css?family=Fjalla+One|Open+Sans:300' rel='stylesheet' type='text/css'>

<link rel="stylesheet" href="https://www.jlab.tech/css/ZGS.min.dd1701f4a8abeef24341ebb7694a1a0186bc33683baf8289decc3b5c4b02115a.css">
    <link rel="shortcut icon" href="/favicon.ico" type="image/x-icon" />
    <link rel="apple-touch-icon" href="/apple-touch-icon.png" />
</head>

<header id="header">
    <figure>
      <a href="/" border=0 id="logolink"><div class="icon-spf13-3" id="logo"> </div></a>
    </figure>
    <nav id="nav">
            <ul id="mainnav">
            <li>
                <a href="/post/">
                <span class="icon"> <i aria-hidden="true" class="icon-quill"></i></span>
                <span> articles </span>
            </a>
            </li>
            <li>
            <a href="/project/">
                <span class="icon"> <i aria-hidden="true" class="icon-console"></i></span>
                <span> projects </span>
            </a>
            </li>
            <li>
            <a href="/presentation/">
                <span class="icon"> <i aria-hidden="true" class="icon-stats"></i></span>
                <span> slides </span>
            </a>
            </li>           
        </ul>

            <ul id="social">
            <li id="follow">
                <span class="title"> follow </span>
                <div class="dropdown follow">
                    <ul class="social">
                        <li> <a href="https://www.twitter.com/jhfnetboy" target="_blank" title="Follow me on Twitter" class="twitter"><span class="icon icon-twitter"></span>Twitter</a> </li>
                        <li> <a href="https://www.linkedin.com/in/%E4%BC%9A%E5%B3%B0-%E7%84%A6-a3483393/" target="_blank" title="LinkedIn" class="linkedin"><span class="icon icon-linkedin"></span>LinkedIn</a> </li>
                        <li> <a href="https://github.com/jhfnetboy" target="_blank" title="GitHub" class="github"><span class="icon icon-github"></span>GitHub</a> </li>
                        <li> <a href="https://www.douban.com/people/gdbme/" target="_blank" title="Douban" class="slideshare"><span class="icon icon-slideshare"></span>Douban</a> </li>
                    </ul>
                    <span class="subcount">Get Creativity &amp; Innovations</span>
                </div>
            </li>
        </ul>

    </nav>
</header>



<section id="main">
  <h1 itemprop="name" id="title">转发 NLP分词算法分析</h1>
  <div>
        <article itemprop="articleBody" id="content">
           <p>自动分词算法的分类</p>
<p>我们可以将现有的分词算法分为三大类：基于字符串匹配的分词方法、基于理解的分词方法和基于统计的分词方法。</p>
<p>1、 基于字符串匹配的分词方法
这种方法又叫做机械分词方法，它是按照一定的策略将待分析的汉字串与一个&quot;充分大的&quot;机器词典中的词条进行配，若在词典中找到某个字符串，则匹配成功（识别出一个词）。</p>
<p>按照扫描方向的不同，串匹配分词方法可以分为正向匹配和逆向匹配；按照不同长度优先匹配的情况，可以分为最大（最长）匹配和最小（最短）匹配；按照是否与词性标注过程相结合，又可以分为单纯分词方法和分词与标注相结合的一体化方法。常用的几种机械分词方法如下；</p>
<p>1） 、正向最大匹配
2） 、逆向最大匹配
3） 、最少切分（使每一句中切出的词数最小）</p>
<p>还可以将上述各种方法相互组合，例如，可以将正向最大匹配方法和逆向最大匹配方法结合起来构成双向匹配法。由于汉语单字成词的特点，正向最小匹配和逆向最小匹配一般很少使用。一般说来，逆向匹配的切分精度略高于正向匹配，遇到的歧义现象也较少。统计结果表明，单纯使用正向最大匹配的错误率为1/169，单纯使用逆向最大匹配的错误率为1/245。（&ndash;这可能是因为汉语的中心语靠后的特点。）但这种精度还远远不能满足实际的需要。由于分词是一个智能决策过程，机械分词方法无法解决分词阶段的两大基本问题：歧义切分问题和未登录词识别问题。实际使用的分词系统，都是把机械分词作为一种初分手段，还需通过利用各种其它的语言信息来进一步提高切分的准确率。</p>
<p>一种方法是改进扫描方式，称为特征扫描或标志切分，优先在待分析字符串中识别和切分出一些带有明显特征的词，以这些词作为断点，可将原字符串分为较小的串再来进机械分词，从而减少匹配的错误率。</p>
<p>另一种方法是将分词和词类标注结合起来，利用丰富的词类信息对分词决策提供帮助，并且在标注过程中又反过来对分词结果进行检验、调整，从而极大地提高切分的准确率。</p>
<p>对于机械分词方法，可以建立一个一般的模型，形式地表示为ASM(d,a,m)，即Automatic Segmentation Model。其中，</p>
<p>d：匹配方向，+1表示正向，-1表示逆向；
a：每次匹配失败后增加/减少字串长度（字符数），+1为增字，-1为减字；
m：最大/最小匹配标志，+1为最大匹配，-1为最小匹配。</p>
<p>例如，ASM(+, -, +)就是正向减字最大匹配法（即MM方法），ASM(-, -, +)就是逆向减字最大匹配法(即RMM方法)，等等。对于现代汉语来说，只有m=+1是实用的方法。</p>
<p>用这种模型可以对各种方法的复杂度进行比较，假设在词典的匹配过程都使用顺序查找和相同的计首字索引查找方法，则在不记首字索引查找次数（最小为log&lt;汉字总数&gt; =12 - 14）和词典读入内存时间的情况下，对于典型的词频分布，减字匹配ASM(d,-,m)的复杂度约为12.3次，增字匹配ASM(d,+,m)的复杂度约为10.6。</p>
<p>另外，还可以证明，早期曾流行一时的&quot;切分标志字串&quot;预处理方法是一个毫无必要的技术,它增加了一遍扫描&quot;切分标志词典&quot;的时空复杂性，却并没有提高分词精度，因为所谓的切分标志其实都已经隐含在词典之中，是对词典功能的重复。实际上&quot;切分标志&quot;也没有标记歧义字段的任何信息。因此，在近来的分词系统中，已经基本上废弃了这种&quot;切分标志&quot;预处理方法。</p>
<p>2、基于理解的分词方法</p>
<p>通常的分析系统，都力图在分词阶段消除所有歧义切分现象。而有些系统则在后续过程中来处理歧义切分问题，其分词过程只是整个语言理解过程的一小部分。其基本思想就是在分词的同时进行句法、语义分析，利用句法信息和语义信息来处理歧义现象。它通常包括三个部分：分词子系统、句法语义子系统、总控部分。在总控部分的协调下，分词子系统可以获得有关词、句子等的句法和语义信息来对分词歧义进行判断，即它模拟了人对句子的理解过程。这种分词方法需要使用大量的语言知识和信息。由于汉语语言知识的笼统、复杂性，难以将各种语言信息组织成机器可直接读取的形式，因此目前基于理解的分词系统还处在试验阶段。</p>
<p>3、基于统计的分词方法</p>
<p>从形式上看，词是稳定的字的组合，因此在上下文中，相邻的字同时出现的次数越多，就越有可能构成一个词。因此字与字相邻共现的频率或概率能够较好的反映成词的可信度。可以对语料中相邻共现的各个字的组合的频度进行统计，计算它们的互现信息。</p>
<p>定义两个字的互现信息为： M(X,Y)=logP(X,Y)/P(X).P(Y)，其中 P(X,Y)是汉字X、Y的相邻共现概率，P(X) 、P(Y)分别是X、Y在语料中出现的概率。互现信息体现了汉字之间结合关系的紧密程度。当紧密程度高于某一个阈值时，便可认为此字组可能构成了一个词。这种方法只需对语料中的字组频度进行统计，不需要切分词典，因而又叫做无词典分词法或统计取词方法。但这种方法也有一定的局限性，会经常抽出一些共现频度高、但并不是词的常用字组，例如&quot;这一&rdquo;、&ldquo;之一&rdquo;、&ldquo;有的&rdquo;、&ldquo;我的&rdquo;、&ldquo;许多的&quot;等，并且对常用词的识别精度差，时空开销大。实际应用的统计分词系统都要使用一部基本的分词词典（常用词词典）进行串匹配分词，同时使用统计方法识别一些新的词，即将串频统计和串匹配结合起来，既发挥匹配分词切分速度快、效率高的特点，又利用了无词典分词结合上下文识别生词、自动消除歧义的优点。</p>
<p>几种典型的自动分词系统评介</p>
<p>衡量自动分词系统的主要指标是切分精度和速度。由于切分速度与所运行的软、硬件平台密切相关，在没有注明运行平台时，切分速度只是一个参考指标，没有可比性。</p>
<p>另外，所注明的切分精度都是开发者自测试的结果。</p>
<p>1、几个早期的自动分词系统
自80年代初中文信息处理领域提出了自动分词以来，一些实用性的分词系统逐步得以开发，其中几个比较有代表性的自动分词系统在当时产生了较大的影响。</p>
<p>CDWS分词系统是我国第一个实用的自动分词系统，由北京航空航天大学计算机系于1983年设计实现，它采用的自动分词方法为最大匹配法，辅助以词尾字构词纠错技术。其分词速度为5-10字/秒，切分精度约为1/625，基本满足了词频统计和其他一些应用的需要。这是汉语自动分词实践的首次尝试，具有很大的启发作用和理论意义。例如，它比较科学地阐明了汉语中的歧义切分字段的类别、特征以及基本的对策（&ndash;切分歧义&quot;标准分类&rdquo;！）。</p>
<p>ABWS是山西大学计算机系研制的自动分词系统，系统使用的分词方法称为&quot;两次扫描联想-回溯&quot;方法，用联想-回溯来解决引起组合切分歧义。系统词库运用了较多的词法、句法等知识。其切分正确率为98.6%(不包括非常用、未登录的专用名词)，运行速度为48词/分钟。</p>
<p>CASS是北京航空航天大学于1９８８年实现的分词系统。它使用的是一种变形的最大匹配方法，即正向增字最大匹配。它运用知识库来处理歧义字段。其机械分词速度为200字/秒以上，知识库分词速度150字/秒（没有完全实现）。</p>
<p>书面汉语自动分词专家系统是由北京师范大学现代教育研究所于1991前后研制实现的，它首次将专家系统方法完整地引入到分词技术中。系统使知识库与推理机保持相对独立，知识库包括常识性知识库（词条的词类24种、歧义词加标志及其消除规则编号、消歧的部分语义知识，使用关联网络存储）和启发性知识库（消歧产生式规则集合，用线性表结构存储），词典使用首字索引数据结构。通过引入专家系统的形式，系统把分词过程表示成为知识的推理过程，即句子&quot;分词树&quot;的生长过程。据报道，系统对封闭原料的切分精度为99.94%，对开放语料的切分精度达到99.8%，在386机器上切分速度达到200字/秒左右。这些性能代表了当时的一流成就。</p>
<p>现在看来，这个系统的一个重要理论意义是进一步研究清楚了歧义切分字段，即把歧义字段分为词法级、句法级、语义级和语用级（&ndash;&ldquo;四级分类&rdquo;），并且统计出它们的分布分别为84.1%、10.8%、3.4%和1.7%，还给出了每一种歧义的处理策略，从而比较彻底地剖析了汉语歧义切分字段的性质。它的另外一个理论意义是给出了当前基于句法和语义处理技术的歧义分析精度的上限（&ldquo;语义级理想切分精度&quot;1/6250），并且说明只有综合运用各种知识、信息和推理机制的分析方法才又可能趋近理想切分精度。尽管本系统由于结构复杂、知识库建造困难且并不像预想的那么易于维护、效率不易提高等原因而未能广泛流行，但是其理论分析和指导思想已获得了普遍关注，影响了众多后继系统的开发。
2、清华大学早期SEG分词系统</p>
<p>此系统提供了带回溯的正向、反向、双向最大匹配法和全切分-评价切分算法，由用户来选择合适的切分算法。其特点则是带修剪的全切分-评价算法。系统考虑到了切分盲点的问题（某些字串永远不会被某种分词方法匹配出来），由此提出了全切分的概念，即找出输入字串的所有可能的子串，然后利用某种评价方法从所有这些可能的子串中选出最佳子串序列作为分词结果。为了解决全切分所带来的组合爆炸问题，又引进了对全切分过程进行修剪的方法，强制性地截止某些全切分的进行。用户在使用时，对于歧义较少的语料，可采用正向或反向最大匹配法；对于有较多交叉歧义的语料，可使用双向最大匹配法；对于其它歧义较大的语料，则采用全切分-评价算法，并需要采用一个合适的评价函数。由于对具体语料的统计参数设置了不确切初值，全切分-评价算法在第一、二遍切分过程中的正确率较低，随着切分的多遍进行，评价函数逐渐得以矫正，系统的切分精度逐步得以提高。经过封闭试验，在多遍切分之后，全切分-评价算法的精度可以达到99%左右。</p>
<p>3、清华大学SEGTAG系统</p>
<p>此系统着眼于将各种各类的信息进行综合，以便最大限度地利用这些信息提高切分精度。系统使用有向图来集成各种各样的信息，这些信息包括切分标志、预切分模式、其他切分单位。为了实现有限的全切分，系统对词典中的每一个重要的词都加上了切分标志，即标志&quot;ck&quot;或&quot;qk&rdquo;。&ldquo;qk&quot;标志表示该词可进行绝对切分，不必理会它是否产生切分歧义；&ldquo;ck&quot;标志表示该词有组合歧义，系统将对其进行全切分，即保留其所有可能的切分方式。
系统通过这两种标志并使用几条规则以实现有限的全切分，限制过多的切分和没有必要的搜索。规则包括：</p>
<p>1、无条件切出qk类词；
2、完全切分ck类词（保留所有可能子串）；
3、对没有标记(qk或ck)的词，若它与别的词之间存在交叉歧义，则作全切分；
否则将其切出。</p>
<p>为了获得切分结果，系统采用在有向图DAG上搜索最佳路径的方法，使用一个评价函数EVALUATE Path)，求此评价函数的极大值而获得最佳路径Pmax。所运用的搜索算法有两种，&ldquo;动态规划&quot;和&quot;全切分搜索+叶子评价&rdquo;，使用了词频、词类频度、词类共现频度等统计信息。通过实验，该系统的切分精度基本上可达到99%左右，能够处理未登录词比较密集的文本,切分速度约为30字/秒。</p>
<p>4、国家语委文字所应用句法分析技术的汉语自动分词</p>
<p>此分词模型考虑了句法分析在自动分词系统中的作用，以更好地解决切分歧义。切词过程考虑到了所有的切分可能，并运用汉语句法等信息从各种切分可能中选择出合理的切分结果。其过程由两步构成：一、对输入字串进行处理，得到一个所有可能的切分字串的集合，即进行（不受限的）全切分；二、利用句法分析从全切分集合中将某些词选出来，由它们构成合理的词序列，还原为原输入字串。系统使用一个自由传播式句法分析网络，用短语文法描述句法规则，并将其表示为层次化网络图，通过此网络的信息传递过程来进行选词。网络的节点分为词类节点（终结符节点）和规则类节点（非终结符节点）。词类节点保存词的信息；规则类节点对信息进行合并和句法、语义分析，生成新的信息，并将本节点的信息传递出去（也就是用文法产生式进行归约，并进行属性计算-作者注）。网络运行的初态是所有节点状态为NO，各种可能切分的字串进入响应相应的词类节点（终结符节点），然后开始运用文法进行计算。当网络的最高层节点S（文法起始符号）达到稳定状态OK时,计算结束，在最高节点处输出最后的切分结果。</p>
<p>从一般的角度来看，应用句法分析技术进行切词的方法是一种&quot;生成-测试&quot;方法，它是一种常用的AI问题求解方法，包括两个步骤：生成步-找出所有可能的解（假设）；测试步-对各个假设进行检验，找出合格者。在应用句法分析进行切词时，其测试步是使用汉语的句法规则检验某种切分结果是否构成合法的汉语句子。这样可以将句法分析理论的各种成果用于切词之中，有多种句法分析技术可以应用，常见的是ATN分析、CYK分析(Chart Parsing)、G-LR分析等。可以将这种方法称做&quot;切词-句法分析一体化&quot;方法。随着软硬件水平的不断提高，直接运用时空消耗比较大的句法分析来检查分词结果的方法正在日益显现其优越性。</p>
<p>5、复旦分词系统</p>
<p>此系统由四个模块构成。</p>
<p>一、预处理模块，利用特殊的标记将输入的文本分割成较短的汉字串，这些标记包括标点符号、数字、字母等非汉字符，还包括文本中常见的一些字体、字号等排版信息。一些特殊的数词短语、时间短语、货币表示等，由于其结构相对简单，即由数词和特征字构成构成，也在本阶段进行处理。为此系统特别增加一次独立的扫描过程来识别这些短语，系统维护一张特征词表，在扫描到特征字以后，即调用这些短语的识别模块，确定这些短语的左、右边界，然后将其完整地切分开；</p>
<p>二、歧义识别模块，使用正向最小匹配和逆向最大匹配对文本进行双向扫描，如果两种扫描结果相同，则认为切分正确，否则就判别其为歧义字段，需要进行歧义处理；</p>
<p>三、歧义字段处理模块，此模块使用构词规则和词频统计信息来进行排歧。构词规则包括前缀、后缀、重叠词等构词情况，以及成语、量词、单字动词切分优先等规则。在使用规则无效的情况下，使用了词频信息，系统取词频的乘积最大的词串作为最后切分结果；最后，此系统还包括一个未登录词识别模块，以解决未登录词造成的分词错误。未登录词和歧义字段构成了降低分词准确率的两大因素，而未登录词造成的切分错误比歧义字段更为严重，实际上绝大多数分词错误都是由未登录词造成的。系统对中文姓氏进行了自动识别，它利用了中文姓名的用字规律、频率，以及姓名的上下文等信息。通过对十万以上的中文姓名进行抽样综合统计，建立了姓氏频率表和名字用字频率表，由此可获得任意相邻的二、三个单字构成姓氏的概率大小和某些规律，再利用这些字串周围的一些称谓、指界动词和特定模式等具有指示意义的上下文信息，对字串是否构成姓名进行辨别。实验过程中，对中文姓氏的自动辨别达到了70%的准确率。系统对文本中的地名和领域专有词汇也进行了一定的识别。</p>
<p>6、哈工大统计分词系统
该系统是一种典型的运用统计方法的纯切词系统，它试图将串频统计和词匹配结合起来。</p>
<p>系统由三个部分构成：</p>
<p>一、预处理模块，利用显式和隐式的切分标记（标点符号、数字、ASCII字符以及出现频率高、构词能力差的单字词、数词+单字常用量词模式）将待分析的文本切分成短的汉字串，这大大地减少了需要统计的（无效）字串的数量和高频单字或量词边界串；</p>
<p>二、串频统计模块，此模块计算各个已分开的短汉字串中所有长度大于１的子串在局部上下文中出现的次数，并根据串频和串长对每个这样的子串进行加权，加权函数为 （F为串频，L为串长，即串中汉字个数）。根据经验，局部上下文中取为200字左右。局部上下文的串频计算使用一个滑动窗口（为一个队列式缓冲区，保存当前待切分汉字串及其前后20个短串），当当前待切分汉字串处理完之后，窗口下移一个短串（中心变为相邻下一个短串）。系统采用一个外散列表来记录窗口中的短串，以加快窗口中串频计数。散列函数取为汉字的GB-80位码（二级汉字共用入口95），每个桶中保存窗口中
每一行（短串）上的汉字位置：（短串的行号，汉字列号），并且对于在窗口中出现多次的汉字位置用一个链指针连接起来，则计算某个字串在窗口中出现的频度时，不必将该字串与窗口中的短串逐个匹配，而只需统计在该字串中的各个汉字所对应的位置链表中能够相邻的位置的序列的个数即可。此外，还需要根据词缀集（前、后缀集合）对字串的权值进行提升，例如&quot;处理器&quot;中&quot;处理&quot;的权值很高，但由于对&quot;处理器&quot;的权值作了提升（达到或超过了&quot;处理&rdquo;），就不会切成&quot;处理/器&rdquo;。如果某个汉字串的权值超过某一阈值D（取为40），则将此汉字串作为一个新识别的词，将其存入一临时词库中；</p>
<p>三、切分模块，首先用临时词库对每个短的汉字串进行切分，使用的是逐词遍历算法，再利用一个小型的常用词词典对汉字短串中未切分的子串进行正向最大匹配分词。对于短汉字串中那些仍未切分的子串，则将所有相邻单字作为一个权值很低的生词（例如&quot;玛&rdquo;、&ldquo;莉&rdquo;）。其中每个模块都对待分析的文本进行了一次扫描，因而是三遍扫描方法。此系统能够利用上下文识别大部分生词，解决一部分切分歧义，但是统计分词方法对常用词识别精度差的固有缺点仍然存在（例如切出&quot;由/来&rdquo;、&ldquo;语/用&rdquo;、&ldquo;对/联&quot;等）。
经测试，此系统的分词错误率为1.5%，速度为236字/秒。
　</p>
<p>7、杭州大学改进的MM分词系统
考虑到汉语的歧义切分字段出现的平均最大概率为1/110，因而纯机械分词的精度在理论上能够达到1-1/110=99.1%。那么是否还有更一般、精度更高的机械分词系统呢？
根据统计，汉语的局部（词法一级）歧义字段占了全部歧义的84%，句法歧义占10%，如果提高系统处理这两类歧义的准确率，则可以大幅度提高切分精度。这方面的改进导致了改进的MM分词算法。将其阐述如下。</p>
<p>通过对交叉歧义字段的考察，发现其中80%以上可以通过运用一条无需任何语言知识的&quot;归右原则&rdquo;（交叉歧义字段优先与其右边的字段成词）就可以获得正确切分，&ndash;这是因为在多数情况下汉语的修饰语在前、中心词在后，因而&quot;归右&quot;好于&quot;归左&rdquo;。 &ldquo;归右原则&quot;可以使机械分词的精度上升到99.70%。这种考察给出了鼓舞人心的结果，有可能使机械分词系统达到这样的理论精度。</p>
<p>不过&quot;归右原则&quot;还有需要修正的地方，既对于&quot;连续型交叉歧义&quot;会发生错误，需要补充一条&quot;左部结合&quot;原则：若ABCDE为连续型交叉歧义字段，&ldquo;归右原则&quot;产生切分A B C D E 再由&quot;左结合原则&rdquo;（合并最左边的A、B）而得到AB C DE。</p>
<p>例如&quot;结合成分子&rdquo;-&gt;&quot;结 合 成 分子&rdquo;-&gt;&quot;结合 成 分子&rdquo;。
但是仍然还有例外，例如&quot;当结合成分子时&rdquo;-&gt;&quot;当 结合 成分 子时&rdquo;；为此引入&quot;跳跃匹配&rdquo;，在词典中定义&quot;非连续词&rdquo;（实际上为串模式-作者注）&ldquo;当*时&rdquo;，然后在切分时首先分出&quot;当 结合成分子 时&rdquo;，然后再用&quot;归右+左结合&quot;切分中间的歧义字段。以上3项技术将机械分词的理论切分精度提高到了99.73%。</p>
<p>综合以上思想，就建立了如下改进的MM分词算法：</p>
<p>正向扫描</p>
<ul>
<li>增字最大匹配（包括&quot;跳跃匹配非连续词&rdquo;）</li>
<li>词尾歧义检查（逐次去掉首字做MM匹配以发现交叉歧义字段）</li>
<li>&ldquo;归右原则&rdquo;（ 对于&quot;连续型交叉歧义&quot;还需要&quot;左结合原则&rdquo;）。</li>
</ul>
<p>系统的词典采用一级首字索引结构，词条中包括了&quot;非连续词&rdquo;（形如C1…* Cn）。系统精
度的实验结果为95%，低于理论值99.73%，但高于通常的MM、RMM、DMM方法。</p>
<p>///bs: 有机会见面 就出了问题；</p>
<p>8、Microsoft Research 汉语句法分析器中的自动分词
微软研究院的自然语言研究所在从90年代初开始开发了一个通用型的多国语言处理平台NLPWin，最初阶段的研究都是对英语进行的。大约从1997年开始，增加了中文处理的研究，从而使NLPWin成为能够进行7国语言处理的系统（其中日语和韩语部分的研究已较早地开展起来）。中文部分的研究在开始时缺少必要的基础资源，于是经过细致的研究分析之后，购买了北大计算语言所的《现代汉语语法信息词典》，从此进展顺利，在短短的一年半的时间里达到了其它东方语种的处理水平。据报道，NLPWin的语法分析部分使用的是一种双向的Chart Parsing，使用了语法规则并以概率模型作导向，并且将语法和分析器独立开。</p>
<p>NLPWin中文部分的一个特点是将词的切分同句法分析融合起来，即是一种前面提到过的&quot;切词-句法分析一体化&quot;方法：在其匹配切词阶段保留所有可能的切分结果（包括歧义切分），然后在句法分析阶段使用汉语的句法规则判断切分的合理性，如果对句子的某种切分能够成功地建立起完全的句法树，则表示该切分结果是正确的。对于有上下文及语用歧义的歧义切分字段，系统将生成两棵以上的分析树（可以使用某种标准进行排序）。</p>
<p>当然，为了提高系统效率，有必要在分词阶段排除尽可能多的局部一级的切分歧义。其中使用的技术有：消除所有导致词典中没有对应词条的单字的切分；为词典中的每一个词条增加一项&quot;Atomic&quot;属性（为1表示不需要分析其内部字串，为0表示需要保留其内部的切分，即是一种组合歧义标志&ndash;作者注）；以及为每个词增加 LeftCond1、RightCond1、LeftCond2、RightCond2 四类字符集合（前两项表示歧义绝对生效，后两项表示歧义有比较高的可能性生效，即歧义的直接前后文&ndash;作者注）；还包括一些排歧规则（例如对于连续型歧义字段ABCD，如果AB和CD不与前后词交叉、A或D
是名词、ABC和BCD都不是词，则切分出AB/CD：&ldquo;昨天下午&rdquo;-&gt;&quot;昨天/下午&rdquo;）。</p>
<p>Bits and Atrributes in Chinese NLPwin (7-9-97)
Bits ……
Attributes ……
For overlapping ambiguity:
LeftClue (a record will not be added to tposrecs if the character on its
left appears in this list.)
RiteClue (a record will not be added to tposrecs if the character on its
right appears in this list.)
LeftHint (a record will be assigned low probability if the character on its
left appears in this list.)
RiteHint (a record will be assigned low probability if the character on its
right appears in this list.)</p>
<p>实验结果表明，系统可以正确处理85%的歧义切分字段，在Pentium 200 PC上的速度约600-900字/秒。考虑到系统对多种切分结果进行了完全的句法分析、对词典每个属性进行了完全的查找，这是相当可观的效率。</p>
<p>我们的评论是: 这是汉语处理的一种有效的综合性途径，值得发扬推广；但这种使用&quot;Atomic&quot;属性的方法实际上只是表示了组合型歧义（占不到1/5）的特征，对更普遍的交叉型歧义（超过总歧义的4/5）的处理还存在效率和效果更好的方法。
　</p>
<p>9、北大计算语言所分词系统
本系统由北京大学计算语言学研究所研制开发，属于分词和词类标注相结合的分词系统。由于将分词和词类标注结合起来，系统可利用丰富的词类信息对分词决策提供帮助，并且在标注过程中又反过来对分词结果进行检验、调整，同时将基于规则的标注排歧与基于语料库统计模型的排歧结合起来，使规则的普遍性与灵活性得到统一，而且对未登入词的估算到达了相当高的准确率。系统的处理过程包括了自动切分和初始词性标记、切分歧义字段识别、组词和标注预处理、词性标记排歧、切分和词性标注后处理等过程，系统的算法综合了多种数据组织和搜索技术，以很低的时空开销实现了高速匹配和查找，同时采用了当代计算语言学的统计方法，运用隐Markov过程进行词类标注和排歧，对算法的效率和稳固性都作了尽可能的优化。此系统的一大特色是对通用性的强调，将最稳定、最常用的4万6千余条现代汉语基本词汇（即将扩充到7万多条）及其有关属性组织成为基本词典，这些词的基本地位都是由汉语语言学家逐一检验认可的，这是本系统通用性的保证；在此词典的基础上充分利用汉语构词法的研究成果，可以识别出大部分的常用词。同时本系统对用户词典机制作了最大限度的扩展，允许用户加入3部到30部以上的自定义词典，并允许用户对这些词典的优先顺序自由排列，这样
就可以用较小规模的多个特殊词典更有针对性地解决具体领域的文本处理。因此本系统的语言模型实现了通用性与多样性的有效结合，并到达了极高的效率。经过最近在搜索算法上的改进，系统的分词连同标注的速度在Pentium 133Hz/16MB内存机器上的达到了每秒3千词以上，而在Pentium II/64MB内存机器上速度高达每秒5千词。自本系统开发以来，已先后向国内和国外十多家单位进行了转让，获得了普遍的好评。</p>
<p>在1998年4月进行的863全国智能接口评测会上，该系统有良好的表现。由于系统对待词的兼类问题的理论观点与评测标准有一些差别，所测得的标注准确率没有达到自测试的水平。该系统的词语分类体系一方面承认汉语词存在兼类现象，一方面又不主张扩大兼类现象，尽量把相同语法功能的词类当作是一个词类，而把词的具体语法属性留到后续过程处理。这些观点与评测标准有所不同。国内还有很多单位开发了分词系统，但大部分都没有参加这一具有极强可比性的评测。</p>
<p>转自：http://blog.csdn.net/cozmic/article/details/659591</p>

        </article>
  </div>
</section>



<aside id="meta">

    <div>
        <section id="datecount">
          <h4 id="date"> Sun Oct 9, 2016 </h4>
          <h5 id="wc"> 300 Words </h5>
          <h5 id="readtime"> Read in about 2 Min </h5>
        </section>
        <ul id="categories">
          
        </ul>
        <ul id="tags">
          
        </ul>
    </div>

    <div>
        <section id="prev">
            &nbsp;<a class="previous" href="https://www.jlab.tech/post/2016-10-07-jhfnetboy-blog1475847467/"><i class="icon-arrow-left"></i> Buy a new domain jLab.tech for this blog ten years</a><br>
        </section><section id="next">
            &nbsp;<a class="next" href="https://www.jlab.tech/post/2016-10-09-jhfnetboy-blog1475984528/">Congratulations! I have passed the China Fund licensing exam <i class="icon-arrow-right"></i></a>
        </section>
    </div>

    <div> <section id="author"> <h4>About the Author:</h4> 

            <p>
            Jason Jiao is an China Software Engineer, Team Leader & Blog Author
            based in HangZhou. He has start up over 3 companies Tech Stack,hold over many times 
            System Refactoring and Reconstruction which all were enterprise level. 
            </p>

            <p>
            He currently works at <a href="http://schoolpal.cn/">Schoolpal</a> on the leadership team 
            of <a href="https://www.xiaobao100.com/sis">Many Product lines</a> where  he is responsible for the tech stack strategy and architecture of
            system and it's over 20 thousands enterprise users and 4M family users.Also he is one of <bold>Core Partner</bold> of company, which was a Highest Governance of this enterprise
            </p>

            <p>
            He previously held Software develope team at <a href="https://www.chinamobileltd.com/">China Mobile</a>, <a
            href="https://jd.com">JD ltd</a> where he
            led multi lines' tech team and surpporting, testing teams, over 1 hundred members. 
            He formerly was holder of the <a href="https://www.jlab.tech">jLab</a>.
            </p>
            
            <p> 
            He is the creator of some of the projects <a href="https://www.jlab.tech/lll">Life Long Learning</a>, <a
                href="ZA.link">ZA</a>, and many <a
            href="http://github.com/jhfnetboy">more</a>.
            </p>
            
            <p>
            Above all of these accomplishments, he has a dog,called Cherry（樱桃）. 
            Outside of technology Jason likes travel, motor bike, reading, and play games.
            </p>

        </section> </div>

</aside>

<meta itemprop="wordCount" content="225">
<meta itemprop="datePublished" content="2016-10-09">
<meta itemprop="url" content="https://www.jlab.tech/post/2016-10-09-jhfnetboy-blog1476008774/">



<footer>
  <div>
    <p>
    &copy; 2013-19 <span itemprop="author" itemscope itemtype="http://schema.org/Person">
        <span itemprop="name">Jason Jiao.</span></span>
    <a href="http://creativecommons.org/licenses/by/3.0/" title="Creative Commons Attribution">Some rights reserved</a>;
    please attribute properly and link back. <br>
    Powered by <a href="http://gohugo.io">Hugo</a>.
    </p>
  </div>
</footer>
<script type="text/javascript">
(function(){var j=function(a,b){return window.getComputedStyle?getComputedStyle(a).getPropertyValue(b):a.currentStyle[b]};var k=function(a,b,c){if(a.addEventListener)a.addEventListener(b,c,false);else a.attachEvent('on'+b,c)};var l=function(a,b){for(key in b)if(b.hasOwnProperty(key))a[key]=b[key];return a};window.fitText=function(d,e,f){var g=l({'minFontSize':-1/0,'maxFontSize':1/0},f);var h=function(a){var b=e||1;var c=function(){a.style.fontSize=Math.max(Math.min(a.clientWidth/(b*10),parseFloat(g.maxFontSize)),parseFloat(g.minFontSize))+'px'};c();k(window,'resize',c)};if(d.length)for(var i=0;i<d.length;i++)h(d[i]);else h(d);return d}})();
fitText(document.getElementById('title'), 1)
</script>

<script type="application/javascript">
var doNotTrack = false;
if (!doNotTrack) {
	window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;
	ga('create', 'UA-161158389-1', 'auto');
	
	ga('send', 'pageview');
}
</script>
<script async src='https://www.google-analytics.com/analytics.js'></script>

</body>
</html>

